<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Dongxiao Li's Homepage</title>
  
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="DongxiaoLi">
    <meta name="keywords" content="Dongxiao Li">
    <meta name="robots" content="index,follow">
    <meta name="description" content="Homepage of Dongxiao Li">
    <link href="https://fonts.googleapis.com/css?family=Lato:100,300,400,700,900" rel="stylesheet">
    <link href="./src/css" rel="stylesheet">
  <link href="./src/bootstrap.min.css" rel="stylesheet" media="screen">
  <link rel="icon" type="image/png" href="https://profiles.stanford.edu/zhihong-chen">
  <style>
  .button {
    background-color: #e7e7e7; color: black;/* Green */
    border: none;
    text-decoration: none;
    display: inline-block;
    margin: 4px 2px;
    cursor: pointer;
  }
</style>
</head>


<body data-new-gr-c-s-check-loaded="14.1147.0" data-gr-ext-installed="">
<div class="container">
<br>
<br>

<div class="row">
<div class="col-sm-4">
<br>
<img class="img-responsive" style="max-height:400px;" src="./src/4.jpg">
</div>

<div class="col-sm-8">
<div class="clearfix visible-xs-block"></div>
<h1>Dongxiao Li</h1>


<div id="pronounceLink" style="display:block;"><p><a href="https://williamleedawn.github.io/#" onclick="$(&#39;#pronounce&#39;).toggle(); return false;"><i>pronunciation</i></a></p></div>

<div id="pronounce" style="display: none;" class="alert">
<p>The first name is pronounced as /Tung Hiu/. I'm more than happy to accept any approximation, so please feel free to use your favorite variants/surrogates!</p>
</div>


<p style="font-size: 14px">Undergraduate<br>
Minzu University of China/City University of Macau
<br>
<b>Email</b>: <tt> 23160149@muc.edu.cn/D23090600680@cityu.edu.mo</tt>.<br>
<a href="https://github.com/williamleedawn" target="_blank">Github</a> <!--, <a href="https://scholar.google.com/citations?user=y55sF8cAAAAJ&hl=en" target="_blank">Google Scholar</a>-->
</p></div>
</div>


<hr>
<h4>
  <a href="https://williamleedawn.github.io/#projects">Projects &amp; Papers </a> /
  <a href="https://williamleedawn.github.io/#competitions">Academic &amp; Non-academic competitions </a> /
  <a href="https://williamleedawn.github.io/#teaching">Teaching &amp; Service</a> /
  <a href="https://williamleedawn.github.io/#awards">Awards</a> 
</h4>
<hr>


<h3 id="bio">About</h3>
<p>
  Hello, I am an undergraduate student in a joint program between Minzu University of China and City University of Macau, with a research focus on the intersection of <b>AI for healthcare</b>, <b>vision-language models</b>, and <b>large language models</b>. My primary interest lies in applying these technologies to cardiovascular applications and computer vision segmentation. 
  </p>
  <p>
  Throughout my academic journey, I have been fortunate to receive guidance from Professors Hao Ren, Fengshi Jing, and Yu Weng, while also collaborating closely with medical professionals such as Chief Physician Wen Jin and Physician Yu Sun from the Second People's Hospital of Guangdong Province, as well as Chief Physician Weibin Cheng from Jinan University Affiliated Provincial Second Hospital and City University of Hong Kong. Their invaluable insights have greatly enhanced my research experience.
  </p>
  <p>
  From January 2024 to September 2024, during my first academic year, I had the honor of leading a young team that made significant progress in both academic and non-academic fields. Within this short period, we achieved notable success:
  </p>
  <ul>
    <li>We won <b>eight awards</b> at the international, national, and provincial levels.</li>
    <li>Our paper <b>"WildTech: Disrupting Wildlife Trafficking with Digital Innovation"</b> earned the Finalist Award in the 2024 American College Students Mathematical Modeling Competition.</li>
    <li>Our project <b>"HeartVista: A Heart Angiography Image Recognition and Segmentation System"</b> won the silver prize at the 2024 China International College Students' Innovation and Entrepreneurship Competition (Hainan Region, Higher Education Main Track).</li>
  </ul>
  <p>
  On the research front, my team and I have submitted <b>five papers to CCF-recommended journals and conferences</b>. One has been accepted by <b>ICA3PP (a CCF-C conference)</b>, while the others focus on local adaptive frameworks for cardiovascular angiography, automation in chest X-ray interpretation, and exploring the relationship between vascular markers and longevity traits in older adults.
  </p>
  <p>
  Our team is committed to excellence in both competitions and academics. Not only have we achieved success in various competitions, but both the <b>top performers in academic GPA</b> and the <b>highest scorers in comprehensive evaluations</b> are members of our group.
  </p>
  <p>
  For undergraduate or graduate students at Minzu University of China, I am open to collaboration on projects or papers related to vision-language models or AI for healthcare. Additionally, I welcome opportunities to work together in academic and non-academic competitions, both within China and internationally.
  </p>
  
  

<h3 id="projects">Projects</h3>
<ul>
  <li>HeartVista: A Heart Angiography Image Recognition and Segmentation System<br>Project Leader<br><!--<a href="https://github.com/FreedomIntelligence/LLMZoo" target="_blank">[Website]</a></li>-->
</ul>


<h3 id="publications">Publications</h3>
<!--
<h4>
<button class="button" onclick="myFunction(&#39;survey&#39;)" style="text-align: left;">
Survey
</button>
</h4>
<ul id="survey" style="display: none;">

  <li><a href="https://arxiv.org/abs/2110.05006" target="_blank">Pre-trained Language Models in Biomedical Domain: A Systematic Survey</a><br>Benyou Wang, Qianqian Xie, Jiahuan Pei, <b>Zhihong Chen</b>, Prayag Tiwari, Zhao Li, Jie Fu<br>ACM Computing Surveys 2023.<br>I am responsible for the content of <b>multi-modality</b>.</li>
  <br>

</ul>

<h4>
<button class="button" onclick="myFunction(&#39;llm&#39;)" style="text-align: left;">
Large Language Models
</button>
</h4>
<ul id="llm" style="display: none;">

  <li><a href="https://arxiv.org/abs/2305.15075" target="_blank">HuatuoGPT, Towards Taming Language Model to Be a Doctor</a><br>Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, <b>Zhihong Chen</b>, Jianquan Li, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan, Benyou Wang, Haizhou Li<br>Findings of EMNLP 2023.</li>
  <br>

</ul>

<h4>
<button class="button" onclick="myFunction(&#39;pretraining&#39;)" style="text-align: left;">
Vision-Language (Representation Learning / Foundation Models)
</button>
</h4>
<ul id="pretraining" style="display: none;">

  <li><a href="https://arxiv.org/abs/2302.08958" target="_blank">Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts</a><br><b>Zhihong Chen</b>*, Shizhe Diao*, Benyou Wang, Guanbin Li and Xiang Wan<br>ICCV 2023.</li>
  <br>

  <li><a href="https://arxiv.org/abs/2209.07118" target="_blank">Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge</a><br><b>Zhihong Chen</b>, Guanbin Li and Xiang Wan<br>ACMMM 2022.</li>
  <br>

  <li><a href="https://arxiv.org/abs/2209.07098" target="_blank">Mapping medical image-text to a joint space via masked modeling</a><br><b>Zhihong Chen</b>, Yuhao Du, Jinpeng Hu, Yang Liu, Guanbin Li, Xiang Wan and Tsung-Hui Chang<br>Medical Image Analysis 2023.</li>
  <br>

  <li><a href="https://arxiv.org/abs/2209.07098" target="_blank">Multi-Modal Masked Autoencoders for Medical Vision-and-Language Pre-Training</a><br><b>Zhihong Chen</b>, Yuhao Du, Jinpeng Hu, Yang Liu, Guanbin Li, Xiang Wan and Tsung-Hui Chang<br>MICCAI 2022.</li>
  <br>

</ul>

<h4>
<button class="button" onclick="myFunction(&#39;specific&#39;)" style="text-align: left;">
Vision-Language (Task-specific Modeling)
</button>
</h4>
<ul id="specific" style="display: none;">

  <li><a href="https://arxiv.org/abs/2211.08584" target="_blank">Toward expanding the scope of radiology report summarization to multiple anatomies and modalities</a><br><b>Zhihong Chen</b>*, Maya Varma*, Xiang Wan, Curtis P. Langlotz and Jean-Benoit Delbrouck*<br>ACL 2023.</li>
  <br>

  <li><a href="https://arxiv.org/abs/2210.08303" target="_blank">Improving Radiology Summarization with Radiograph and Anatomy Prompts</a><br>Jinpeng Hu, <b>Zhihong Chen</b>, Yang Liu, Xiang Wan, and Tsung-Hui Chang<br>Findings of ACL 2023.</li>
  <br>

  <li><a href="https://arxiv.org/abs/2204.13258" target="_blank">Cross-modal Memory Networks for Radiology Report Generation</a><br><b>Zhihong Chen</b>, Yaling Shen, Yan Song and Xiang Wan<br>ACL 2021.</li>
  <br>

  <li><a href="https://arxiv.org/abs/2010.16056" target="_blank">Generating Radiology Reports via Memory-driven Transformer</a><br><b>Zhihong Chen</b>, Yan Song, Tsung-Hui Chang and Xiang Wan<br>EMNLP 2020.</li>
  <br>

  <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/4924" target="_blank">Hierarchical Attention Network for Image Captioning</a><br>Weixuan Wang, <b>Zhihong Chen</b> and Haifeng Hu<br>AAAI 2019.</li>
  <br>

</ul>

<h4>
<button class="button" onclick="myFunction(&#39;analysis&#39;)" style="text-align: left;">
Vision-Language (Adaptation, Analysis, etc.)
</button>
</h4>
<ul id="analysis" style="display: none;">

  <li><a href="https://arxiv.org/abs/2307.11545" target="_blank">Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation</a><br>Zunnan Xu*, <b>Zhihong Chen</b>*, Yong Zhang, Yibing Song, Xiang Wan and Guanbin Li<br>ICCV 2023.</li>
  <br>

  <li><a href="https://arxiv.org/abs/2306.03678" target="_blank">On the Difference of BERT-style and CLIP-style Text Encoders</a><br><b>Zhihong Chen</b>*, Guiming Hardy Chen*, Shizhe Diao, Xiang Wan and Benyou Wang<br>Findings of ACL 2023.</li>
  <br>

  <li><a href="https://arxiv.org/abs/2307.11558" target="_blank">Advancing Visual Grounding with Scene Knowledge: Benchmark and Method</a><br><b>Zhihong Chen</b>*, Ruifei Zhang*, Yibin Song, Xiang Wan and Guanbin Li<br>CVPR 2023.</li>
  <br>

</ul>


<h4>
<button class="button" onclick="myFunction(&#39;others&#39;)" style="text-align: left;">
Other Papers
</button>
</h4>
<ul id="others" style="display:none;">

<li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/26069" target="_blank">EASAL: Entity-Aware Subsequence-based Active Learning for Named Entity Recognition</a><br>Yang Liu, Jinpeng Hu, <b>Zhihong Chen</b>, Xiang Wan, and Tsung-Hui Chang<br>AAAI 2023.</li>
<br>

<li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/26515" target="_blank">A Simple yet Effective Subsequence-Enhanced Approach for Cross-Domain NER</a><br>Jinpeng Hu, Dandan Guo, Yang Liu, Zhuo Li, <b>Zhihong Chen</b>, Xiang Wan, and Tsung-Hui Chang<br>AAAI 2023.</li>
<br>

<li><a href="https://arxiv.org/abs/2204.00203" target="_blank">Graph Enhanced Contrastive Learning for Radiology Findings Summarization</a><br>Jinpeng Hu, Zhuo Li, <b>Zhihong Chen</b>, Zhen Li, Xiang Wan and Tsung-Hui Chang<br>ACL 2022.</li>
<br>

<li><a href="https://arxiv.org/abs/2112.09925" target="_blank">Word Graph Guided Summarization for Radiology Findings</a><br>Jinpeng Hu, Jianling Li, <b>Zhihong Chen</b>, Yaling Shen, Yan Song, Xiang Wan and Tsung-Hui Chang<br>Findings of ACL 2021.</li>
<br>

</ul>
-->
<h3 id="competitions">Competitions</h3>

<h4>
<button class="button" onclick="myFunction(&#39;survey&#39;)" style="text-align: left;">
Mathematical Contest in Modeling
</button>
</h4>
<ul id="modeling" style="display: none;">
  
<li><a href="" target="_blank">The Mathematical Contest in Modeling (MCM)®&The Interdisciplinary Contest in Modeling (ICM)®, 2024</a><br>Xinyue Zhang, Xinyuan Tian, <b>Dongxiao Li</b><br>WildTech: Disrupting Wildlife Trafficking with
      Digital Innovation.<br>Modeling Engineer</li>
<br>
  
  

<li>Modeling Engineer, The Mathematical Contest in Modeling (MCM)®&The Interdisciplinary Contest in Modeling (ICM)®, Finalist, 2024</li>
  <!--
  <li>Teaching Assistant, MAT1001 Calculus, Fall 2020 (Outstanding TA Award)</li>
  <li>Teaching Assistant, STA2001 Probability and Statistics, Spring 2020</li>
  <li>Teaching Assistant, MAT1001 Calculus, Fall 2019</li>
  -->
</ul>

<h3 id="teaching">Teaching</h3>
<ul>
  <li>Student, Linear Algebra, 98, Spring 2024</li>
  <li>Student, Fundamental of Programing, 99, Spring 2024</li>
  <li>Student, Object-oriented Programming, 100, Fall 2024</li>
  <!--
  <li>Teaching Assistant, MAT1001 Calculus, Fall 2020 (Outstanding TA Award)</li>
  <li>Teaching Assistant, STA2001 Probability and Statistics, Spring 2020</li>
  <li>Teaching Assistant, MAT1001 Calculus, Fall 2019</li>
  -->
</ul>


<h3 id="service">Service</h3>
<ul>
  <li>Deputy Secretary of the Youth League Committee, Hainan International College, 2024</li>
  <!--
  <li>Reviewer: ACL (2021; 2022; 2023; 2024), EMNLP (2020; 2022; 2023), NAACL (2022; 2024), COLING (2020), ARR (2021; 2022; 2023), CVPR (2024), TMI (2023)</li>
  -->
</ul>


<h3 id="awards">Awards</h3>
<ul>
  <li>Professional first-class scholarship, 2024</li>
  <!--
  <li>Presidential Fellowship, 2019-2024</li>
  <li>MICCAI Student Travel Award, 2023</li>
  <li>Outstanding Teaching Assistant (TA) Award, 2021</li>
  <li>National Scholarship, 2018</li>
  -->
</ul>


<script>
function myFunction(name){
  var x = document.getElementById(name)
  if (x.style.display === "none"){
    x.style.display = null
  } else {
    x.style.display = "none"
  }
}
</script>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3QKWLN0PN5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-3QKWLN0PN5');
</script>

<hr>
<p>Last update: 2024/09. Template credited to <a href="https://zhjohnchan.github.io/#projects">Zhihong Chen</a>'s, <a href="https://ai.stanford.edu/~tengyuma/">Tengyu Ma</a>'s and <a href="https://www.cs.princeton.edu/~danqic">Danqi Chen</a>'s.</p>
<script src="./src/jquery.min.js"></script>
</div>
</html>
